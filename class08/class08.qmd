---
title: "Class 08: Miniproject"
author: "Christopher Brockie (PID: A16280405)"
format: pdf
---
Today we are applying machine learning to breast cancer biopsy data from fine needle aspiration (FNA).

First I put the .csv file into the class08 file on my computer. Then I call it up and rename it:

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```

Now we want to omit the first column, which is the diagnosis.
Save your input data file into your Project directory
```{r}
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

We are saving the diagnosis column for later, as a factor.
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
```

> Q1. How many people are in this data set?

```{r}
nrow(wisc.data)
```

>Q2. How many of the observations have the malignant diagnosis?

```{r}
table( wisc.df$diagnosis )
```

```{r}
sum(wisc.df$diagnosis == "M")
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
x <- colnames(wisc.df)
length( grep("_mean", x))
x
```

# Principal Component Analysis

We need to scale our input data before PCA as some of the columns are measured in terms of very different units with different means and different variances. The upshot here is we set `scale=TRUE` argument to `prcomp()`.

```{r}
wisc.pr <- prcomp( wisc.data, scale=TRUE)
summary(wisc.pr)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

*0.4427*

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

*3*

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

*7* 

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```
*This plot appears to have three-dimensions. It is very difficult to read because it plots each patient ID, which becomes jumbled up in the middle. It is also difficult to tell the values without points.*

Generate one of our main result figures - the PC plot (a.k.a. "score plot", "orientation plot", "PC1 vs PC2 plot","PC plot", "projection plot", etc.) It is known by different names.
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, pch=16, xlab="PC1", ylab="PC2")
```

And a ggplot version
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis, pch=16, xlab="PC1", ylab="PC3")
```
*This "PC1 vs. PC3" plot appears to have more overlap between the red and black dots. This means that it captures less variance than PC2, as the "PC1 vs. PC2" plot had more defined subgroups.*

Calculate the variance of each principal component by squaring the sdev component of wisc.pr using `wisc.pr$sdev^2`. Save the result as an object called `pr.var`.
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called `pve` and create a plot of variance explained for each principal component.
```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
> Q9. For the first principal component, what is the component of the loading vector `wisc.pr$rotation[,1]` for the feature `concave.points_mean`?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
sum(pve[1:5])
```
*5 principal components are needed*

## Hierarchical clustering

The goal of this section is to do hierarchical clustering of the original data. As part of the preparation for hierarchical clustering, the distance between all pairs of observations are computed. Furthermore, there are different ways to link clusters together, with single, complete, and average being the most common linkage methods.

First scale the `wisc.data` data and assign the result to `data.scaled`.
```{r}
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to `data.dist`.
```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to `hclust()` and assign the results to `wisc.hclust`.
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

## Selecting number of clusters

Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable `wisc.hclust.clusters`.
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
wisc.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
```

We can use the table() function to compare the cluster membership to the actual diagnoses.
```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters4 <- cutree(wisc.pr.hclust, k=4)
table(wisc.hclust.clusters4, diagnosis)
```

As we discussed in our last class videos there are number of different “methods” we can use to combine points during the hierarchical clustering procedure. These include "single", "complete", "average" and (my favorite) "ward.D2".

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning. 

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```
*This method minimizes variance within the clusters, making a single group where all points are included at the top of the three. It makes for a much more understandable and visually appealing graph.*

## K-means clustering

Create a k-means model on `wisc.data`, assigning the result to `wisc.km`. Be sure to create 2 clusters, corresponding to the actual number of diagnosis. Also, remember to scale the data (with the `scale()` function and repeat the algorithm 20 times (by setting setting the value of the nstart argument appropriately). Running multiple times such as this will help to find a well performing model.
```{r}
wisc.km <- kmeans(scale(wisc.data), centers=2, nstart=20)
```

Use the `table()` function to compare the cluster membership of the k-means model `wisc.km$cluster` to the actual diagnoses contained in the diagnosis vector.
```{r}
table(wisc.km$cluster, diagnosis)
```
> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```



## 5. Combining methods

This approach will take not the data but our PCA results and work with them.
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object.
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
grps
```


```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```

Note the color swap here as the hclust cluster 1 is mostly “M” and cluster 2 is mostly “B” as we saw from the results of calling table(grps, diagnosis). To match things up we can turn our groups into a factor and reorder the levels so cluster 2 comes first and thus gets the first color (black) and cluster 1 gets the second color (red).

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

Plot using our re-ordered factor:
```{r}
plot(wisc.pr$x[,1:2], col=g)
```

We can be fancy and look in 3D with the rgl or plotly packages. Note that this output will not work well with PDF format reports yet so feel free to skip this optional step for your PDF report. If you have difficulty installing the rgl package on mac then you will likely need to install the XQuartz package from here: https://www.xquartz.org. There are also lots of other packages (like plotly) that can make interactive 3D plots.

```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
```

Use the distance along the first 7 PCs for clustering.
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
```

Cut this hierarchical clustering model into 2 clusters.
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

Using table(), compare the results from your new hierarchical clustering model with the actual diagnoses.
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```


